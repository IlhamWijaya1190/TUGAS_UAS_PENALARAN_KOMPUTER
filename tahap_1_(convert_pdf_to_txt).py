# -*- coding: utf-8 -*-
"""TAHAP1-(CONVERT PDF TO TXT).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PucuUaQjAuX9qghA3FKaFFvnvf6oDrnf
"""

import os
import re
import time
import logging
import requests
from pathlib import Path
from urllib.parse import urljoin, urlparse
from datetime import datetime
import json

# PDF processing
try:
    from pdfminer.high_level import extract_text
    from pdfminer.pdfparser import PDFSyntaxError
    PDF_AVAILABLE = True
except ImportError:
    print("Warning: pdfminer not available. Install with: pip install pdfminer.six")
    PDF_AVAILABLE = False

# HTML processing
try:
    from bs4 import BeautifulSoup
    HTML_AVAILABLE = True
except ImportError:
    print("Warning: BeautifulSoup not available. Install with: pip install beautifulsoup4")
    HTML_AVAILABLE = False

class LegalCaseBaseScraper:
    def __init__(self, base_dir="legal_cases"):
        self.base_dir = Path(base_dir)
        self.data_dir = self.base_dir / "data" / "raw"
        self.logs_dir = self.base_dir / "logs"

        # Create directories
        self.data_dir.mkdir(parents=True, exist_ok=True)
        self.logs_dir.mkdir(parents=True, exist_ok=True)

        # Setup logging
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(self.logs_dir / "cleaning.log"),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)

        # Session for requests
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })

        self.cases_processed = 0
        self.cases_failed = 0
        self.processing_stats = []

    def download_document(self, url, filename):
        """Download a document from URL"""
        try:
            self.logger.info(f"Downloading: {url}")
            response = self.session.get(url, timeout=30)
            response.raise_for_status()

            filepath = self.data_dir / "downloads" / filename
            filepath.parent.mkdir(exist_ok=True)

            with open(filepath, 'wb') as f:
                f.write(response.content)

            self.logger.info(f"Downloaded: {filename}")
            return filepath

        except Exception as e:
            self.logger.error(f"Failed to download {url}: {str(e)}")
            return None

    def extract_text_from_pdf(self, pdf_path):
        """Extract text from PDF file"""
        if not PDF_AVAILABLE:
            raise ImportError("pdfminer not available")

        try:
            text = extract_text(str(pdf_path))
            return text
        except PDFSyntaxError as e:
            self.logger.error(f"PDF syntax error in {pdf_path}: {str(e)}")
            return None
        except Exception as e:
            self.logger.error(f"Error extracting text from {pdf_path}: {str(e)}")
            return None

    def extract_text_from_html(self, html_path):
        """Extract text from HTML file"""
        if not HTML_AVAILABLE:
            raise ImportError("BeautifulSoup not available")

        try:
            with open(html_path, 'r', encoding='utf-8') as f:
                content = f.read()

            soup = BeautifulSoup(content, 'html.parser')

            # Remove script and style elements
            for script in soup(["script", "style"]):
                script.decompose()

            # Extract text
            text = soup.get_text()
            return text

        except Exception as e:
            self.logger.error(f"Error extracting text from {html_path}: {str(e)}")
            return None

    def clean_text(self, text, case_id):
        """Clean and normalize extracted text"""
        if not text:
            return ""

        original_length = len(text)

        # Remove common headers/footers patterns
        header_patterns = [
            r'MAHKAMAH AGUNG.*?\n',
            r'DIREKTORI PUTUSAN.*?\n',
            r'Halaman \d+ dari \d+',
            r'Page \d+ of \d+',
            r'www\.mahkamahagung\.go\.id.*?\n',
        ]

        for pattern in header_patterns:
            text = re.sub(pattern, '', text, flags=re.IGNORECASE | re.MULTILINE)

        # Remove watermarks and repetitive patterns
        watermark_patterns = [
            r'SALINAN.*?SALINAN',
            r'COPY.*?COPY',
            r'\*{5,}.*?\*{5,}',
            r'-{10,}',
            r'={10,}',
        ]

        for pattern in watermark_patterns:
            text = re.sub(pattern, '', text, flags=re.IGNORECASE | re.DOTALL)

        # Normalize whitespace
        text = re.sub(r'\s+', ' ', text)  # Multiple spaces to single space
        text = re.sub(r'\n\s*\n', '\n\n', text)  # Multiple newlines to double newline

        # Convert to lowercase if needed (optional for legal documents)
        # text = text.lower()

        # Remove excessive punctuation
        text = re.sub(r'[.]{3,}', '...', text)

        # Trim whitespace
        text = text.strip()

        cleaned_length = len(text)
        reduction_percentage = ((original_length - cleaned_length) / original_length) * 100

        self.logger.info(f"Case {case_id}: Text cleaned. "
                        f"Original: {original_length} chars, "
                        f"Cleaned: {cleaned_length} chars "
                        f"({reduction_percentage:.1f}% reduction)")

        return text

    def validate_text_quality(self, text, case_id, min_length=1000):
        """Validate the quality of extracted text"""
        if not text:
            self.logger.warning(f"Case {case_id}: Empty text")
            return False, "Empty text"

        if len(text) < min_length:
            self.logger.warning(f"Case {case_id}: Text too short ({len(text)} chars)")
            return False, f"Text too short ({len(text)} chars)"

        # Check for minimum content indicators
        legal_indicators = [
            'putusan', 'hakim', 'pengadilan', 'tergugat', 'penggugat',
            'dakwaan', 'terdakwa', 'jaksa', 'pemohon', 'termohon'
        ]

        found_indicators = sum(indicator in text.lower() for indicator in legal_indicators)
        indicator_ratio = found_indicators / len(legal_indicators)

        if indicator_ratio < 0.3:  # At least 30% of legal indicators should be present
            self.logger.warning(f"Case {case_id}: Low legal content indicators ({indicator_ratio:.1%})")
            return False, f"Low legal content indicators ({indicator_ratio:.1%})"

        # Check text coherence (basic)
        sentences = text.split('.')
        avg_sentence_length = sum(len(s.split()) for s in sentences) / len(sentences) if sentences else 0

        if avg_sentence_length < 5:
            self.logger.warning(f"Case {case_id}: Suspicious text structure")
            return False, "Suspicious text structure"

        self.logger.info(f"Case {case_id}: Text validation passed. "
                        f"Length: {len(text)}, "
                        f"Legal indicators: {indicator_ratio:.1%}")

        return True, "Valid"

    def process_document(self, document_path, case_id):
        """Process a single document (PDF or HTML)"""
        try:
            file_ext = document_path.suffix.lower()

            # Extract text based on file type
            if file_ext == '.pdf':
                raw_text = self.extract_text_from_pdf(document_path)
            elif file_ext in ['.html', '.htm']:
                raw_text = self.extract_text_from_html(document_path)
            else:
                self.logger.error(f"Unsupported file type: {file_ext}")
                return False

            if not raw_text:
                self.logger.error(f"Failed to extract text from {document_path}")
                return False

            # Clean text
            cleaned_text = self.clean_text(raw_text, case_id)

            # Validate text quality
            is_valid, validation_msg = self.validate_text_quality(cleaned_text, case_id)

            # Save cleaned text
            output_path = self.data_dir / f"case_{case_id:03d}.txt"
            with open(output_path, 'w', encoding='utf-8') as f:
                f.write(cleaned_text)

            # Record processing statistics
            stats = {
                'case_id': case_id,
                'source_file': str(document_path),
                'output_file': str(output_path),
                'original_length': len(raw_text),
                'cleaned_length': len(cleaned_text),
                'is_valid': is_valid,
                'validation_message': validation_msg,
                'processed_at': datetime.now().isoformat()
            }

            self.processing_stats.append(stats)

            if is_valid:
                self.cases_processed += 1
                self.logger.info(f"Successfully processed case {case_id}")
            else:
                self.cases_failed += 1
                self.logger.warning(f"Case {case_id} processed but validation failed: {validation_msg}")

            return True

        except Exception as e:
            self.logger.error(f"Error processing case {case_id}: {str(e)}")
            self.cases_failed += 1
            return False

    def process_batch_documents(self, document_paths):
        """Process a batch of documents"""
        self.logger.info(f"Starting batch processing of {len(document_paths)} documents")

        for i, doc_path in enumerate(document_paths, 1):
            self.logger.info(f"Processing document {i}/{len(document_paths)}: {doc_path}")
            self.process_document(Path(doc_path), i)

            # Add small delay to be respectful
            time.sleep(0.5)

        self.generate_processing_report()

    def generate_processing_report(self):
        """Generate a processing report"""
        report = {
            'summary': {
                'total_documents': len(self.processing_stats),
                'successfully_processed': self.cases_processed,
                'failed_processing': self.cases_failed,
                'success_rate': (self.cases_processed / len(self.processing_stats)) * 100 if self.processing_stats else 0
            },
            'processing_details': self.processing_stats,
            'generated_at': datetime.now().isoformat()
        }

        # Save report as JSON
        report_path = self.logs_dir / "processing_report.json"
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)

        self.logger.info(f"Processing complete. Report saved to {report_path}")
        self.logger.info(f"Summary: {self.cases_processed} successful, {self.cases_failed} failed")

def main():
    """Main function to demonstrate usage"""
    scraper = LegalCaseBaseScraper()

    print("Legal Case Base Builder - Stage 1")
    print("=" * 50)
    print()

    # Example usage - you would replace this with actual document paths
    sample_documents = [
        # Add your document paths here
        # "/path/to/case1.pdf",
        # "/path/to/case2.html",
        # "/path/to/case3.pdf",
    ]

    if not sample_documents:
        print("To use this scraper:")
        print("1. Install required packages:")
        print("   pip install pdfminer.six beautifulsoup4 requests")
        print()
        print("2. Collect your legal documents (PDF/HTML files)")
        print()
        print("3. Update the sample_documents list with your file paths")
        print()
        print("4. Run the script")
        print()
        print("Directory structure will be created:")
        print("legal_cases/")
        print("├── data/")
        print("│   └── raw/")
        print("│       ├── case_001.txt")
        print("│       ├── case_002.txt")
        print("│       └── ...")
        print("└── logs/")
        print("    ├── cleaning.log")
        print("    └── processing_report.json")

        return

    # Process documents
    scraper.process_batch_documents(sample_documents)

    print(f"\nProcessing complete!")
    print(f"Check the logs at: {scraper.logs_dir}")
    print(f"Cleaned cases saved to: {scraper.data_dir}")

if __name__ == "__main__":
    main()