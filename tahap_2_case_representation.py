# -*- coding: utf-8 -*-
"""TAHAP 2-Case Representation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rBQIJqVwQMNTNKnTz7XAzpR6_81Emuur
"""

import os
import re
import json
import csv
import logging
from pathlib import Path
from datetime import datetime
from dataclasses import dataclass, asdict
from typing import List, Dict, Optional, Tuple
import pandas as pd
from collections import Counter
import string

# For advanced text processing
try:
    import nltk
    from nltk.tokenize import word_tokenize, sent_tokenize
    from nltk.corpus import stopwords
    NLTK_AVAILABLE = True
except ImportError:
    print("Warning: NLTK not available. Install with: pip install nltk")
    NLTK_AVAILABLE = False

@dataclass
class LegalCaseStructure:
    """Data structure for a legal case"""
    case_id: int
    no_perkara: str
    tanggal: str
    jenis_perkara: str
    tingkat_pengadilan: str
    nama_pengadilan: str
    majelis_hakim: str
    pihak_penggugat: str
    pihak_tergugat: str
    pihak_lain: str
    pasal_terkait: List[str]
    ringkasan_fakta: str
    posita: str
    petitum: str
    pertimbangan_hukum: str
    amar_putusan: str
    text_full: str
    word_count: int
    sentence_count: int
    keywords: List[str]
    legal_principles: List[str]
    file_source: str
    processed_at: str

class LegalCaseStructurer:
    def __init__(self, base_dir="legal_cases"):
        self.base_dir = Path(base_dir)
        self.raw_dir = self.base_dir / "data" / "raw"
        self.processed_dir = self.base_dir / "data" / "processed"
        self.logs_dir = self.base_dir / "logs"

        # Create directories
        self.processed_dir.mkdir(parents=True, exist_ok=True)

        # Setup logging
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(self.logs_dir / "structuring.log"),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)

        # Indonesian stopwords (basic set)
        self.indonesian_stopwords = {
            'yang', 'dan', 'di', 'ke', 'dari', 'pada', 'untuk', 'dengan', 'dalam', 'oleh',
            'adalah', 'akan', 'telah', 'sudah', 'dapat', 'harus', 'bisa', 'maka', 'jika',
            'bahwa', 'atau', 'serta', 'juga', 'karena', 'sebab', 'agar', 'supaya', 'ketika',
            'saat', 'waktu', 'setelah', 'sebelum', 'selama', 'hingga', 'sampai', 'antara'
        }

        # Legal keywords for Indonesian law
        self.legal_keywords = {
            'contract': ['kontrak', 'perjanjian', 'kesepakatan', 'persetujuan'],
            'breach': ['wanprestasi', 'ingkar', 'lalai', 'cedera'],
            'damages': ['ganti rugi', 'kerugian', 'kompensasi', 'denda'],
            'civil': ['perdata', 'gugatan', 'tergugat', 'penggugat'],
            'criminal': ['pidana', 'terdakwa', 'dakwaan', 'jaksa'],
            'court': ['pengadilan', 'hakim', 'majelis', 'putusan'],
            'law': ['undang', 'pasal', 'ayat', 'kuhper', 'kuhap']
        }

        self.cases_processed = 0
        self.cases_failed = 0
        self.structured_cases = []

    def extract_case_number(self, text: str) -> str:
        """Extract case number from text"""
        patterns = [
            r'No(?:mor)?\.?\s*(?:Perkara)?\s*:?\s*(\d+/[A-Za-z\./]+/\d{4})',
            r'Nomor\s+Putusan\s*:?\s*(\d+/[A-Za-z\./]+/\d{4})',
            r'Perkara\s+(?:No|Nomor)\.?\s*(\d+/[A-Za-z\./]+/\d{4})',
            r'(\d+/Pdt\.G/\d{4}/[A-Za-z\.-]+)',
            r'(\d+/Pid\.B/\d{4}/[A-Za-z\.-]+)',
            r'(\d+/[A-Za-z]+\.[A-Za-z]+/\d{4}/[A-Za-z\.-]+)'
        ]

        for pattern in patterns:
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                return match.group(1).strip()

        return "Tidak ditemukan"

    def extract_date(self, text: str) -> str:
        """Extract decision date from text"""
        patterns = [
            r'(?:tanggal|tgl|pada)\s*:?\s*(\d{1,2}[-/]\d{1,2}[-/]\d{4})',
            r'(?:tanggal|tgl|pada)\s*:?\s*(\d{1,2}\s+[A-Za-z]+\s+\d{4})',
            r'Jakarta,?\s*(\d{1,2}\s+[A-Za-z]+\s+\d{4})',
            r'(\d{1,2}\s+[A-Za-z]+\s+\d{4})'
        ]

        for pattern in patterns:
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                return match.group(1).strip()

        return "Tidak ditemukan"

    def extract_case_type(self, text: str) -> str:
        """Extract case type from text"""
        if re.search(r'pdt\.g|perdata|gugatan', text, re.IGNORECASE):
            return "Perdata"
        elif re.search(r'pid\.b|pidana|dakwaan', text, re.IGNORECASE):
            return "Pidana"
        elif re.search(r'pdt\.p|permohonan', text, re.IGNORECASE):
            return "Permohonan"
        elif re.search(r'tun|administrasi', text, re.IGNORECASE):
            return "Tata Usaha Negara"
        else:
            return "Lainnya"

    def extract_court_info(self, text: str) -> Tuple[str, str]:
        """Extract court level and court name"""
        # Court level
        if re.search(r'mahkamah agung|ma', text, re.IGNORECASE):
            level = "Mahkamah Agung"
        elif re.search(r'pengadilan tinggi|pt', text, re.IGNORECASE):
            level = "Pengadilan Tinggi"
        elif re.search(r'pengadilan negeri|pn', text, re.IGNORECASE):
            level = "Pengadilan Negeri"
        else:
            level = "Tidak ditemukan"

        # Court name
        court_patterns = [
            r'pengadilan\s+(?:negeri|tinggi)\s+([A-Za-z\s]+)',
            r'pn\s+([A-Za-z\s]+)',
            r'pt\s+([A-Za-z\s]+)'
        ]

        court_name = "Tidak ditemukan"
        for pattern in court_patterns:
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                court_name = match.group(1).strip()
                break

        return level, court_name

    def extract_parties(self, text: str) -> Tuple[str, str, str]:
        """Extract parties involved in the case"""
        # Penggugat (Plaintiff)
        penggugat_patterns = [
            r'penggugat\s*:?\s*([A-Za-z\s,\.]+?)(?:alamat|tempat|vs|melawan|tergugat)',
            r'pemohon\s*:?\s*([A-Za-z\s,\.]+?)(?:alamat|tempat|vs|melawan|termohon)',
            r'([A-Za-z\s,\.]+?)\s+(?:selaku|sebagai)\s+penggugat'
        ]

        penggugat = "Tidak ditemukan"
        for pattern in penggugat_patterns:
            match = re.search(pattern, text, re.IGNORECASE | re.DOTALL)
            if match:
                penggugat = match.group(1).strip()[:100]  # Limit length
                break

        # Tergugat (Defendant)
        tergugat_patterns = [
            r'tergugat\s*:?\s*([A-Za-z\s,\.]+?)(?:alamat|tempat|bahwa|yang)',
            r'termohon\s*:?\s*([A-Za-z\s,\.]+?)(?:alamat|tempat|bahwa|yang)',
            r'melawan\s+([A-Za-z\s,\.]+?)(?:alamat|tempat|bahwa|yang)'
        ]

        tergugat = "Tidak ditemukan"
        for pattern in tergugat_patterns:
            match = re.search(pattern, text, re.IGNORECASE | re.DOTALL)
            if match:
                tergugat = match.group(1).strip()[:100]  # Limit length
                break

        # Other parties
        other_parties = []
        other_patterns = [
            r'turut\s+tergugat\s*:?\s*([A-Za-z\s,\.]+?)(?:alamat|tempat|bahwa)',
            r'tergugat\s+intervensi\s*:?\s*([A-Za-z\s,\.]+?)(?:alamat|tempat|bahwa)'
        ]

        for pattern in other_patterns:
            matches = re.findall(pattern, text, re.IGNORECASE | re.DOTALL)
            other_parties.extend([match.strip()[:50] for match in matches])

        pihak_lain = "; ".join(other_parties) if other_parties else "Tidak ada"

        return penggugat, tergugat, pihak_lain

    def extract_legal_articles(self, text: str) -> List[str]:
        """Extract legal articles referenced in the case"""
        patterns = [
            r'pasal\s+(\d+(?:\s+(?:jo|juncto|junto)\s+pasal\s+\d+)*)\s+(?:kuh|kitab|undang)',
            r'pasal\s+(\d+)\s+(?:ayat\s+\(\d+\)\s+)?(?:kuh|kitab)',
            r'undang[-\s]undang\s+(?:no|nomor)\.?\s*(\d+)\s+tahun\s+(\d+)',
            r'uu\s+(?:no|nomor)\.?\s*(\d+/\d+)',
            r'peraturan\s+pemerintah\s+(?:no|nomor)\.?\s*(\d+)\s+tahun\s+(\d+)'
        ]

        articles = []
        for pattern in patterns:
            matches = re.findall(pattern, text, re.IGNORECASE)
            for match in matches:
                if isinstance(match, tuple):
                    articles.append(" ".join(match))
                else:
                    articles.append(match)

        return list(set(articles))  # Remove duplicates

    def extract_key_sections(self, text: str) -> Dict[str, str]:
        """Extract key sections of the legal decision"""
        sections = {
            'ringkasan_fakta': '',
            'posita': '',
            'petitum': '',
            'pertimbangan_hukum': '',
            'amar_putusan': ''
        }

        # Define section patterns
        section_patterns = {
            'ringkasan_fakta': [
                r'fakta[-\s]fakta\s+(?:yang\s+)?terungkap\s*:?\s*(.*?)(?=menimbang|pertimbangan|bahwa)',
                r'duduk\s+perkara\s*:?\s*(.*?)(?=menimbang|pertimbangan|bahwa)',
                r'kronologi\s*:?\s*(.*?)(?=menimbang|pertimbangan|bahwa)'
            ],
            'posita': [
                r'(?:bahwa\s+)?penggugat\s+(?:mendalilkan|menyatakan|menguraikan)\s*(.*?)(?=petitum|mohon|menimbang)',
                r'posita\s*:?\s*(.*?)(?=petitum|mohon|menimbang)'
            ],
            'petitum': [
                r'petitum\s*:?\s*(.*?)(?=menimbang|pertimbangan|bahwa)',
                r'mohon\s+(?:agar\s+)?(?:pengadilan|majelis)\s*(.*?)(?=menimbang|pertimbangan|bahwa)'
            ],
            'pertimbangan_hukum': [
                r'menimbang\s*:?\s*(.*?)(?=mengadili|memutuskan|amar)',
                r'pertimbangan\s+hukum\s*:?\s*(.*?)(?=mengadili|memutuskan|amar)'
            ],
            'amar_putusan': [
                r'(?:mengadili|memutuskan|amar)\s*:?\s*(.*?)$',
                r'putusan\s*:?\s*(.*?)$'
            ]
        }

        for section_name, patterns in section_patterns.items():
            for pattern in patterns:
                match = re.search(pattern, text, re.IGNORECASE | re.DOTALL)
                if match:
                    content = match.group(1).strip()
                    # Limit length and clean
                    sections[section_name] = content[:1000] if content else ''
                    break

        return sections

    def extract_judges(self, text: str) -> str:
        """Extract judges' names"""
        patterns = [
            r'majelis\s+hakim\s*:?\s*(.*?)(?=panitera|clerk|yang|bahwa)',
            r'hakim\s+(?:ketua|anggota)\s*:?\s*(.*?)(?=panitera|clerk|yang|bahwa)',
            r'(?:ketua|hakim)\s*:?\s*([A-Za-z\s,\.]+?)(?:panitera|clerk|anggota)'
        ]

        for pattern in patterns:
            match = re.search(pattern, text, re.IGNORECASE | re.DOTALL)
            if match:
                judges = match.group(1).strip()
                return judges[:200]  # Limit length

        return "Tidak ditemukan"

    def calculate_text_features(self, text: str) -> Tuple[int, int, List[str]]:
        """Calculate text features and extract keywords"""
        # Word count
        words = text.split()
        word_count = len(words)

        # Sentence count
        sentences = re.split(r'[.!?]+', text)
        sentence_count = len([s for s in sentences if s.strip()])

        # Extract keywords (remove stopwords and get most frequent)
        if NLTK_AVAILABLE:
            try:
                # Tokenize and remove stopwords
                words_lower = [word.lower() for word in word_tokenize(text)]
                words_filtered = [
                    word for word in words_lower
                    if word not in self.indonesian_stopwords
                    and word not in string.punctuation
                    and len(word) > 2
                ]

                # Get most common words
                word_freq = Counter(words_filtered)
                keywords = [word for word, freq in word_freq.most_common(20)]

            except:
                # Fallback method
                words_lower = [word.lower() for word in words if len(word) > 2]
                word_freq = Counter(words_lower)
                keywords = [word for word, freq in word_freq.most_common(20)]
        else:
            # Simple keyword extraction
            words_lower = [word.lower() for word in words if len(word) > 2]
            word_freq = Counter(words_lower)
            keywords = [word for word, freq in word_freq.most_common(20)]

        return word_count, sentence_count, keywords

    def extract_legal_principles(self, text: str) -> List[str]:
        """Extract legal principles and concepts"""
        principles = []

        # Search for legal concepts
        for category, keywords in self.legal_keywords.items():
            for keyword in keywords:
                if keyword in text.lower():
                    principles.append(f"{category}:{keyword}")

        # Extract specific legal principles
        principle_patterns = [
            r'asas\s+([A-Za-z\s]+)',
            r'prinsip\s+([A-Za-z\s]+)',
            r'doktrin\s+([A-Za-z\s]+)'
        ]

        for pattern in principle_patterns:
            matches = re.findall(pattern, text, re.IGNORECASE)
            principles.extend([f"principle:{match.strip()}" for match in matches])

        return list(set(principles))  # Remove duplicates

    def process_single_case(self, file_path: Path, case_id: int) -> Optional[LegalCaseStructure]:
        """Process a single case file and extract structured data"""
        try:
            self.logger.info(f"Processing case {case_id}: {file_path.name}")

            # Read the cleaned text
            with open(file_path, 'r', encoding='utf-8') as f:
                text_full = f.read()

            if not text_full.strip():
                self.logger.warning(f"Case {case_id}: Empty file")
                return None

            # Extract all components
            no_perkara = self.extract_case_number(text_full)
            tanggal = self.extract_date(text_full)
            jenis_perkara = self.extract_case_type(text_full)
            tingkat_pengadilan, nama_pengadilan = self.extract_court_info(text_full)
            majelis_hakim = self.extract_judges(text_full)
            pihak_penggugat, pihak_tergugat, pihak_lain = self.extract_parties(text_full)
            pasal_terkait = self.extract_legal_articles(text_full)
            key_sections = self.extract_key_sections(text_full)
            word_count, sentence_count, keywords = self.calculate_text_features(text_full)
            legal_principles = self.extract_legal_principles(text_full)

            # Create structured case
            case = LegalCaseStructure(
                case_id=case_id,
                no_perkara=no_perkara,
                tanggal=tanggal,
                jenis_perkara=jenis_perkara,
                tingkat_pengadilan=tingkat_pengadilan,
                nama_pengadilan=nama_pengadilan,
                majelis_hakim=majelis_hakim,
                pihak_penggugat=pihak_penggugat,
                pihak_tergugat=pihak_tergugat,
                pihak_lain=pihak_lain,
                pasal_terkait=pasal_terkait,
                ringkasan_fakta=key_sections['ringkasan_fakta'],
                posita=key_sections['posita'],
                petitum=key_sections['petitum'],
                pertimbangan_hukum=key_sections['pertimbangan_hukum'],
                amar_putusan=key_sections['amar_putusan'],
                text_full=text_full,
                word_count=word_count,
                sentence_count=sentence_count,
                keywords=keywords,
                legal_principles=legal_principles,
                file_source=str(file_path),
                processed_at=datetime.now().isoformat()
            )

            self.logger.info(f"Successfully structured case {case_id}")
            self.cases_processed += 1
            return case

        except Exception as e:
            self.logger.error(f"Error processing case {case_id}: {str(e)}")
            self.cases_failed += 1
            return None

    def process_all_cases(self):
        """Process all cases in the raw directory"""
        case_files = list(self.raw_dir.glob("case_*.txt"))
        self.logger.info(f"Found {len(case_files)} case files to process")

        if not case_files:
            self.logger.warning("No case files found in raw directory")
            return

        for i, file_path in enumerate(case_files, 1):
            case = self.process_single_case(file_path, i)
            if case:
                self.structured_cases.append(case)

        self.logger.info(f"Processing complete: {self.cases_processed} successful, {self.cases_failed} failed")

    def save_to_csv(self):
        """Save structured cases to CSV format"""
        csv_path = self.processed_dir / "cases.csv"

        try:
            with open(csv_path, 'w', newline='', encoding='utf-8') as csvfile:
                if not self.structured_cases:
                    self.logger.warning("No cases to save")
                    return

                # Get field names from dataclass
                fieldnames = list(asdict(self.structured_cases[0]).keys())
                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)

                writer.writeheader()
                for case in self.structured_cases:
                    case_dict = asdict(case)
                    # Convert lists to strings for CSV
                    for key, value in case_dict.items():
                        if isinstance(value, list):
                            case_dict[key] = "; ".join(map(str, value))
                    writer.writerow(case_dict)

            self.logger.info(f"Cases saved to CSV: {csv_path}")

        except Exception as e:
            self.logger.error(f"Error saving to CSV: {str(e)}")

    def save_to_json(self):
        """Save structured cases to JSON format"""
        json_path = self.processed_dir / "cases.json"

        try:
            cases_data = {
                'metadata': {
                    'total_cases': len(self.structured_cases),
                    'processed_at': datetime.now().isoformat(),
                    'processing_stats': {
                        'successful': self.cases_processed,
                        'failed': self.cases_failed
                    }
                },
                'cases': [asdict(case) for case in self.structured_cases]
            }

            with open(json_path, 'w', encoding='utf-8') as f:
                json.dump(cases_data, f, indent=2, ensure_ascii=False)

            self.logger.info(f"Cases saved to JSON: {json_path}")

        except Exception as e:
            self.logger.error(f"Error saving to JSON: {str(e)}")

    def generate_summary_report(self):
        """Generate a summary report of the structured data"""
        if not self.structured_cases:
            self.logger.warning("No cases available for summary")
            return

        # Basic statistics
        case_types = Counter([case.jenis_perkara for case in self.structured_cases])
        court_levels = Counter([case.tingkat_pengadilan for case in self.structured_cases])
        avg_word_count = sum(case.word_count for case in self.structured_cases) / len(self.structured_cases)

        # Generate report
        report = {
            'summary': {
                'total_cases': len(self.structured_cases),
                'case_types': dict(case_types),
                'court_levels': dict(court_levels),
                'average_word_count': round(avg_word_count, 2),
                'date_range': {
                    'earliest': min([case.tanggal for case in self.structured_cases if case.tanggal != "Tidak ditemukan"], default="Unknown"),
                    'latest': max([case.tanggal for case in self.structured_cases if case.tanggal != "Tidak ditemukan"], default="Unknown")
                }
            },
            'quality_metrics': {
                'cases_with_case_number': sum(1 for case in self.structured_cases if case.no_perkara != "Tidak ditemukan"),
                'cases_with_date': sum(1 for case in self.structured_cases if case.tanggal != "Tidak ditemukan"),
                'cases_with_parties': sum(1 for case in self.structured_cases if case.pihak_penggugat != "Tidak ditemukan"),
                'cases_with_legal_articles': sum(1 for case in self.structured_cases if case.pasal_terkait)
            }
        }

        # Save report
        report_path = self.processed_dir / "summary_report.json"
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)

        self.logger.info(f"Summary report saved: {report_path}")

        # Print summary
        print("\n" + "="*50)
        print("LEGAL CASE STRUCTURING SUMMARY")
        print("="*50)
        print(f"Total cases processed: {len(self.structured_cases)}")
        print(f"Case types: {dict(case_types)}")
        print(f"Court levels: {dict(court_levels)}")
        print(f"Average word count: {avg_word_count:.0f}")
        print(f"Success rate: {(self.cases_processed/(self.cases_processed + self.cases_failed))*100:.1f}%")

def main():
    """Main function to run the legal case structurer"""
    structurer = LegalCaseStructurer()

    print("Legal Case Structurer - Stage 2")
    print("=" * 50)

    # Process all cases
    structurer.process_all_cases()

    if structurer.structured_cases:
        # Save in both formats
        structurer.save_to_csv()
        structurer.save_to_json()

        # Generate summary report
        structurer.generate_summary_report()

        print(f"\nFiles created:")
        print(f"- CSV: {structurer.processed_dir}/cases.csv")
        print(f"- JSON: {structurer.processed_dir}/cases.json")
        print(f"- Summary: {structurer.processed_dir}/summary_report.json")
        print(f"- Log: {structurer.logs_dir}/structuring.log")
    else:
        print("\nNo cases were successfully processed. Check the logs for details.")

if __name__ == "__main__":
    main()